{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libs\n",
    "import os\n",
    "import pandas as pd\n",
    "from processos import  EXTRATOR_CNPJ\n",
    "from pyspark.sql.functions import concat_ws, lpad, coalesce, when, lit, col, date_format\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "from typing import List\n",
    "from time import localtime, strftime\n",
    "current_dir = os.getcwd()\n",
    "dir_dados = r\"\\dados\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Começando a buscar os dados: 30/08/2023 15:49:06\n",
      "Termino da coleta dos ESTABELECIMENTOS: 30/08/2023 15:50:54\n",
      "Termino da coleta dos EMPRESAS: 30/08/2023 15:51:43\n",
      "Final da coleta dos MUNICIPIOS: 30/08/2023 15:51:43\n",
      "Final da coleta dos dados: 30/08/2023 15:51:43\n"
     ]
    }
   ],
   "source": [
    "# Se passar baixar_e_extrair como false, precisa do nome do arquivo.\n",
    "print(f'Começando a buscar os dados: {strftime(\"%d/%m/%Y %H:%M:%S\", localtime())}')\n",
    "ESTABELECIMENTOS, spark = EXTRATOR_CNPJ(baixar_e_extrair=False, nome_arquivo=\"Estabelecimentos\").run()\n",
    "\n",
    "print(f'Termino da coleta dos ESTABELECIMENTOS: {strftime(\"%d/%m/%Y %H:%M:%S\", localtime())}')\n",
    "EMPRESAS, spark = EXTRATOR_CNPJ(baixar_e_extrair=False, nome_arquivo=\"Empresas\").run()\n",
    "\n",
    "print(f'Termino da coleta dos EMPRESAS: {strftime(\"%d/%m/%Y %H:%M:%S\", localtime())}')\n",
    "MUNICIPIOS, spark = EXTRATOR_CNPJ(baixar_e_extrair=False, nome_arquivo=\"Municipios\").run()\n",
    "\n",
    "print(f'Final da coleta dos MUNICIPIOS: {strftime(\"%d/%m/%Y %H:%M:%S\", localtime())}')\n",
    "CNAES, spark = EXTRATOR_CNPJ(baixar_e_extrair=False, nome_arquivo=\"Cnaes\").run()\n",
    "\n",
    "print(f'Final da coleta dos dados: {strftime(\"%d/%m/%Y %H:%M:%S\", localtime())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, explode\n",
    "# Cria uma nova coluna com os valores da coluna CNAE_SECUNDARIO divididos em uma lista\n",
    "ESTABELECIMENTOS = ESTABELECIMENTOS.withColumn(\"CNAE_SECUNDARIO_LIST\", split(ESTABELECIMENTOS[\"CNAE_SECUNDARIO\"], \",\"))\n",
    "# Explode a coluna CNAE_SECUNDARIO_LIST para criar uma linha para cada valor\n",
    "ESTABELECIMENTOS = ESTABELECIMENTOS.select(\"*\", explode(\"CNAE_SECUNDARIO_LIST\").alias(\"CNAE_SECUNDARIO_VALUE\"))\n",
    "\n",
    "# Cria uma view com o mesmo nome do DataFrame\n",
    "ESTABELECIMENTOS.createOrReplaceTempView(\"ESTABELECIMENTOS\")\n",
    "EMPRESAS.createOrReplaceTempView(\"EMPRESAS\")\n",
    "MUNICIPIOS.createOrReplaceTempView(\"MUNICIPIOS\")\n",
    "CNAES.createOrReplaceTempView(\"CNAES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNAES_FILTROS = {\n",
    "        5620104:'Fornecimento de alimentos preparados preponderantemente para consumo domiciliar',\n",
    "        5611201:'Restaurantes e similares',\n",
    "        5611203:'Lanchonetes casas de chá de sucos e similares',\n",
    "        5611204:'Bares e outros estabelecimentos especializados em servir bebidas sem entretenimento',\n",
    "        5611205:'Bares e outros estabelecimentos especializados em servir bebidas com entretenimento',\n",
    "        4721102:'Padaria e confeitaria com predominância de revenda'\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtra_cnpj_cnae_principal():\n",
    "    dados = spark.sql(\n",
    "        \"\"\"SELECT\n",
    "          CONCAT(LPAD(EMPRESAS.CNPJ_BASE, 8, '0'), LPAD(ESTABELECIMENTOS.CNPJ_ORDEM, 4, '0'), LPAD(ESTABELECIMENTOS.CNPJ_DV, 2, '0')) AS CNPJ,\n",
    "          EMPRESAS.RAZAO_SOCIAL,\n",
    "          ESTABELECIMENTOS.NOME_FANTASIA,\n",
    "          ESTABELECIMENTOS.SITUACAO_CADASTRAL,\n",
    "                    date(\n",
    "            substr(\n",
    "              ESTABELECIMENTOS.DATA_SITUACAO_CADASTRAL, 1, 4\n",
    "            ) || '-' || \n",
    "            substr(\n",
    "              ESTABELECIMENTOS.DATA_SITUACAO_CADASTRAL, 5, 2\n",
    "            ) || '-' || \n",
    "            substr(\n",
    "              ESTABELECIMENTOS.DATA_SITUACAO_CADASTRAL, 7, 2\n",
    "            )\n",
    "          ) AS DATA_SITUACAO_CADASTRAL,\n",
    "          date(\n",
    "            substr(\n",
    "                ESTABELECIMENTOS.DATA_INICIO_ATIVIDADE, 1, 4\n",
    "              ) || '-' || \n",
    "            substr(\n",
    "                ESTABELECIMENTOS.DATA_INICIO_ATIVIDADE, 5, 2\n",
    "              ) || '-' || \n",
    "            substr(\n",
    "                ESTABELECIMENTOS.DATA_INICIO_ATIVIDADE, 7, 2\n",
    "              )\n",
    "          ) AS DATA_INICIO_ATIVIDADE,\n",
    "          ESTABELECIMENTOS.CNAE_PRINCIPAL,\n",
    "          CNAES.DESCRICAO_CNAE,\n",
    "          ESTABELECIMENTOS.CNAE_SECUNDARIO,\n",
    "          CONCAT(\n",
    "            COALESCE(ESTABELECIMENTOS.TIPO_LOGRADOURO, ''),\n",
    "            ' ',\n",
    "            COALESCE(ESTABELECIMENTOS.LOGRADOURO, ''),\n",
    "            ' ',\n",
    "            COALESCE(ESTABELECIMENTOS.NUMERO, ''),\n",
    "            ' ',\n",
    "            COALESCE(ESTABELECIMENTOS.COMPLEMENTO, '')\n",
    "          ) AS ENDERECO,\n",
    "          ESTABELECIMENTOS.BAIRRO,\n",
    "          MUNICIPIOS.NOME_MUNICIPIO AS CIDADE,\n",
    "          ESTABELECIMENTOS.UF,\n",
    "          ESTABELECIMENTOS.CEP,\n",
    "          CONCAT(\n",
    "            COALESCE(ESTABELECIMENTOS.DDD_CONTATO, ''),\n",
    "            ' ',\n",
    "            COALESCE(ESTABELECIMENTOS.TELEFONE_CONTATO, '')\n",
    "          ) AS TELEFONE,\n",
    "          ESTABELECIMENTOS.EMAIL\n",
    "        FROM EMPRESAS\n",
    "        LEFT JOIN ESTABELECIMENTOS ON ESTABELECIMENTOS.CNPJ_BASE = EMPRESAS.CNPJ_BASE\n",
    "        LEFT JOIN MUNICIPIOS ON ESTABELECIMENTOS.CODIGO_MUNICIPIO = MUNICIPIOS.CODIGO_MUNICIPIO\n",
    "        LEFT JOIN CNAES ON ESTABELECIMENTOS.CNAE_PRINCIPAL = CNAES.CODIGO_CNAE\n",
    "        WHERE ESTABELECIMENTOS.CNAE_PRINCIPAL IN ('4721102', '5611201', '5611203', '5611204', '5611205',  '5612100', '5620104') -- AND ESTABELECIMENTOS.SITUACAO_CADASTRAL IN (2, 3, 4)\n",
    "        ORDER BY ESTABELECIMENTOS.CNPJ_BASE\n",
    "    \"\"\")\n",
    "    return dados\n",
    "\n",
    "def filtra_cnpj_cnae_segundario():\n",
    "    dados = spark.sql(\n",
    "        \"\"\"SELECT\n",
    "          CONCAT(LPAD(EMPRESAS.CNPJ_BASE, 8, '0'), LPAD(ESTABELECIMENTOS.CNPJ_ORDEM, 4, '0'), LPAD(ESTABELECIMENTOS.CNPJ_DV, 2, '0')) AS CNPJ,\n",
    "          EMPRESAS.RAZAO_SOCIAL,\n",
    "          ESTABELECIMENTOS.NOME_FANTASIA,\n",
    "          ESTABELECIMENTOS.SITUACAO_CADASTRAL,\n",
    "          date(\n",
    "            substr(\n",
    "              ESTABELECIMENTOS.DATA_SITUACAO_CADASTRAL, 1, 4\n",
    "            ) || '-' || \n",
    "            substr(\n",
    "              ESTABELECIMENTOS.DATA_SITUACAO_CADASTRAL, 5, 2\n",
    "            ) || '-' || \n",
    "            substr(\n",
    "              ESTABELECIMENTOS.DATA_SITUACAO_CADASTRAL, 7, 2\n",
    "            )\n",
    "          ) AS DATA_SITUACAO_CADASTRAL,\n",
    "          date(\n",
    "            substr(\n",
    "              ESTABELECIMENTOS.DATA_INICIO_ATIVIDADE, 1, 4\n",
    "              ) || '-' || \n",
    "            substr(\n",
    "              ESTABELECIMENTOS.DATA_INICIO_ATIVIDADE, 5, 2\n",
    "              ) || '-' || \n",
    "            substr(\n",
    "              ESTABELECIMENTOS.DATA_INICIO_ATIVIDADE, 7, 2\n",
    "              )\n",
    "          ) AS DATA_INICIO_ATIVIDADE,\n",
    "          ESTABELECIMENTOS.CNAE_PRINCIPAL,\n",
    "          CNAES.DESCRICAO_CNAE,\n",
    "          ESTABELECIMENTOS.CNAE_SECUNDARIO,\n",
    "          CONCAT(\n",
    "            COALESCE(ESTABELECIMENTOS.TIPO_LOGRADOURO, ''),\n",
    "            ' ',\n",
    "            COALESCE(ESTABELECIMENTOS.LOGRADOURO, ''),\n",
    "            ' ',\n",
    "            COALESCE(ESTABELECIMENTOS.NUMERO, ''),\n",
    "            ' ',\n",
    "            COALESCE(ESTABELECIMENTOS.COMPLEMENTO, '')\n",
    "          ) AS ENDERECO,\n",
    "          ESTABELECIMENTOS.BAIRRO,\n",
    "          MUNICIPIOS.NOME_MUNICIPIO AS CIDADE,\n",
    "          ESTABELECIMENTOS.UF,\n",
    "          ESTABELECIMENTOS.CEP,\n",
    "          CONCAT(\n",
    "            COALESCE(ESTABELECIMENTOS.DDD_CONTATO, ''),\n",
    "            ' ',\n",
    "            COALESCE(ESTABELECIMENTOS.TELEFONE_CONTATO, '')\n",
    "          ) AS TELEFONE,\n",
    "          ESTABELECIMENTOS.EMAIL\n",
    "        FROM EMPRESAS\n",
    "        LEFT JOIN ESTABELECIMENTOS ON ESTABELECIMENTOS.CNPJ_BASE = EMPRESAS.CNPJ_BASE\n",
    "        LEFT JOIN MUNICIPIOS ON ESTABELECIMENTOS.CODIGO_MUNICIPIO = MUNICIPIOS.CODIGO_MUNICIPIO\n",
    "        LEFT JOIN CNAES ON ESTABELECIMENTOS.CNAE_PRINCIPAL = CNAES.CODIGO_CNAE\n",
    "        WHERE ESTABELECIMENTOS.CNAE_SECUNDARIO_VALUE IN ('4721102', '5611201', '5611203', '5611204', '5611205',  '5612100', '5620104') -- AND ESTABELECIMENTOS.SITUACAO_CADASTRAL IN (2, 3, 4)\n",
    "        ORDER BY ESTABELECIMENTOS.CNPJ_BASE\n",
    "    \"\"\")\n",
    "    return dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função de unir vários dataframes\n",
    "def unir_dataframes(lista_df: List) -> DataFrame:\n",
    "    # Cria uma união por nome das colunas\n",
    "    unir_dataframes_por_colunas_diferentes = lambda dfa, dfb: dfa.unionByName(dfb, allowMissingColumns=True)\n",
    "    \n",
    "    # use reduce to combine all the dataframes\n",
    "    dataframe_final = reduce(unir_dataframes_por_colunas_diferentes, lista_df)\n",
    "    \n",
    "    return dataframe_final\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o302.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 46.0 failed 1 times, most recent failure: Lost task 4.0 in stage 46.0 (TID 174) (ntb-46 executor driver): java.io.IOException: Espaço insuficiente no disco\r\n\tat java.io.FileOutputStream.writeBytes(Native Method)\r\n\tat java.io.FileOutputStream.write(Unknown Source)\r\n\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\r\n\tat org.apache.spark.io.MutableCheckedOutputStream.write(MutableCheckedOutputStream.scala:43)\r\n\tat java.io.BufferedOutputStream.flushBuffer(Unknown Source)\r\n\tat java.io.BufferedOutputStream.write(Unknown Source)\r\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\r\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\r\n\tat java.io.BufferedOutputStream.flushBuffer(Unknown Source)\r\n\tat java.io.BufferedOutputStream.write(Unknown Source)\r\n\tat java.io.DataOutputStream.write(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:543)\r\n\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:69)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:310)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: java.io.IOException: Espaço insuficiente no disco\r\n\tat java.io.FileOutputStream.writeBytes(Native Method)\r\n\tat java.io.FileOutputStream.write(Unknown Source)\r\n\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\r\n\tat org.apache.spark.io.MutableCheckedOutputStream.write(MutableCheckedOutputStream.scala:43)\r\n\tat java.io.BufferedOutputStream.flushBuffer(Unknown Source)\r\n\tat java.io.BufferedOutputStream.write(Unknown Source)\r\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\r\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\r\n\tat java.io.BufferedOutputStream.flushBuffer(Unknown Source)\r\n\tat java.io.BufferedOutputStream.write(Unknown Source)\r\n\tat java.io.DataOutputStream.write(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:543)\r\n\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:69)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:310)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 27\u001b[0m\n\u001b[0;32m     21\u001b[0m dados_primario \u001b[39m=\u001b[39m dados_primario\u001b[39m.\u001b[39mselect(col(\u001b[39m\"\u001b[39m\u001b[39mCNPJ\u001b[39m\u001b[39m\"\u001b[39m), col(\u001b[39m\"\u001b[39m\u001b[39mRAZAO_SOCIAL\u001b[39m\u001b[39m\"\u001b[39m), col(\u001b[39m\"\u001b[39m\u001b[39mNOME_FANTASIA\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m     22\u001b[0m                      col(\u001b[39m\"\u001b[39m\u001b[39mSITUACAO_CADASTRAL\u001b[39m\u001b[39m\"\u001b[39m), col(\u001b[39m\"\u001b[39m\u001b[39mDATA_SITUACAO_CADASTRAL\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m     23\u001b[0m                      col(\u001b[39m\"\u001b[39m\u001b[39mDATA_INICIO_ATIVIDADE\u001b[39m\u001b[39m\"\u001b[39m), col(\u001b[39m\"\u001b[39m\u001b[39mCNAE_PRINCIPAL\u001b[39m\u001b[39m\"\u001b[39m), col(\u001b[39m\"\u001b[39m\u001b[39mENDERECO\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m     24\u001b[0m                      col(\u001b[39m\"\u001b[39m\u001b[39mBAIRRO\u001b[39m\u001b[39m\"\u001b[39m), col(\u001b[39m\"\u001b[39m\u001b[39mCIDADE\u001b[39m\u001b[39m\"\u001b[39m), col(\u001b[39m\"\u001b[39m\u001b[39mUF\u001b[39m\u001b[39m\"\u001b[39m), col(\u001b[39m\"\u001b[39m\u001b[39mCEP\u001b[39m\u001b[39m\"\u001b[39m), col(\u001b[39m\"\u001b[39m\u001b[39mTELEFONE\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m     25\u001b[0m                      col(\u001b[39m\"\u001b[39m\u001b[39mCNAE_DESCRICAO\u001b[39m\u001b[39m\"\u001b[39m), col(\u001b[39m\"\u001b[39m\u001b[39mEMAIL\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m     26\u001b[0m \u001b[39m# Salva o dataframe inicial\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m dados_pandas_primario \u001b[39m=\u001b[39m dados_primario\u001b[39m.\u001b[39;49mtoPandas()\n\u001b[0;32m     28\u001b[0m dados_pandas_primario\u001b[39m.\u001b[39mto_csv(arquivo_csv, sep\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m;\u001b[39m\u001b[39m'\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     29\u001b[0m dados_pandas_primario\u001b[39m.\u001b[39mto_parquet(ar, sep\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m;\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ABRASEL NACIONAL\\Documents\\GitHub\\ESTABELECIMENTOS_AFL\\.venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:208\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    205\u001b[0m             \u001b[39mraise\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[39m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m pdf \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame\u001b[39m.\u001b[39mfrom_records(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect(), columns\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns)\n\u001b[0;32m    209\u001b[0m column_counter \u001b[39m=\u001b[39m Counter(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns)\n\u001b[0;32m    211\u001b[0m corrected_dtypes: List[Optional[Type]] \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m] \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mschema)\n",
      "File \u001b[1;32mc:\\Users\\ABRASEL NACIONAL\\Documents\\GitHub\\ESTABELECIMENTOS_AFL\\.venv\\lib\\site-packages\\pyspark\\sql\\dataframe.py:1216\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1196\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[0;32m   1197\u001b[0m \n\u001b[0;32m   1198\u001b[0m \u001b[39m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1213\u001b[0m \u001b[39m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[0;32m   1214\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[39mwith\u001b[39;00m SCCallSiteSync(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sc):\n\u001b[1;32m-> 1216\u001b[0m     sock_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mcollectToPython()\n\u001b[0;32m   1217\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[1;32mc:\\Users\\ABRASEL NACIONAL\\Documents\\GitHub\\ESTABELECIMENTOS_AFL\\.venv\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\ABRASEL NACIONAL\\Documents\\GitHub\\ESTABELECIMENTOS_AFL\\.venv\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    168\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    170\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\ABRASEL NACIONAL\\Documents\\GitHub\\ESTABELECIMENTOS_AFL\\.venv\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o302.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 46.0 failed 1 times, most recent failure: Lost task 4.0 in stage 46.0 (TID 174) (ntb-46 executor driver): java.io.IOException: Espaço insuficiente no disco\r\n\tat java.io.FileOutputStream.writeBytes(Native Method)\r\n\tat java.io.FileOutputStream.write(Unknown Source)\r\n\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\r\n\tat org.apache.spark.io.MutableCheckedOutputStream.write(MutableCheckedOutputStream.scala:43)\r\n\tat java.io.BufferedOutputStream.flushBuffer(Unknown Source)\r\n\tat java.io.BufferedOutputStream.write(Unknown Source)\r\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\r\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\r\n\tat java.io.BufferedOutputStream.flushBuffer(Unknown Source)\r\n\tat java.io.BufferedOutputStream.write(Unknown Source)\r\n\tat java.io.DataOutputStream.write(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:543)\r\n\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:69)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:310)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: java.io.IOException: Espaço insuficiente no disco\r\n\tat java.io.FileOutputStream.writeBytes(Native Method)\r\n\tat java.io.FileOutputStream.write(Unknown Source)\r\n\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\r\n\tat org.apache.spark.io.MutableCheckedOutputStream.write(MutableCheckedOutputStream.scala:43)\r\n\tat java.io.BufferedOutputStream.flushBuffer(Unknown Source)\r\n\tat java.io.BufferedOutputStream.write(Unknown Source)\r\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\r\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\r\n\tat java.io.BufferedOutputStream.flushBuffer(Unknown Source)\r\n\tat java.io.BufferedOutputStream.write(Unknown Source)\r\n\tat java.io.DataOutputStream.write(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:543)\r\n\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:69)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:310)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "# Importa a função de backup\n",
    "from backup_limpeza import backup_limpeza_simples\n",
    "\n",
    "# Mapeia o caminho para salvar o arquivo, verifica se já existe dados lá e faz o backup\n",
    "arquivo_csv = os.path.join(dir_dados, r\"csv\\BASE_RFB.csv\")\n",
    "arquivo_parquet = os.path.join(dir_dados, r\"parquet\\BASE_RFB.parquet\")\n",
    "\n",
    "if os.path.exists(arquivo_csv):\n",
    "    nome_backup = dir_dados + r\"/backup/\"\n",
    "    if not os.path.exists(nome_backup):\n",
    "        os.makedirs(nome_backup)\n",
    "    backup_limpeza_simples(pasta=arquivo_csv.replace(r\"BASE_RFB.csv\", \"\"), nome_zipado=nome_backup + f\"BASE_RFB_{strftime('%d-%m-%Y %H_%M_%S', localtime())}.zip\")\n",
    "\n",
    "# Chama a função de filtrar por cnae\n",
    "dados_primario = filtra_cnpj_cnae_principal()\n",
    "\n",
    "# Renomeia a coluna CNAE_DESCRICAO\n",
    "dados_primario = dados_primario.withColumnRenamed(\"DESCRICAO_CNAE\", \"CNAE_DESCRICAO\")\n",
    "\n",
    "# Seleciona as colunas\n",
    "dados_primario = dados_primario.select(col(\"CNPJ\"), col(\"RAZAO_SOCIAL\"), col(\"NOME_FANTASIA\"),\n",
    "                     col(\"SITUACAO_CADASTRAL\"), col(\"DATA_SITUACAO_CADASTRAL\"),\n",
    "                     col(\"DATA_INICIO_ATIVIDADE\"), col(\"CNAE_PRINCIPAL\"), col(\"ENDERECO\"),\n",
    "                     col(\"BAIRRO\"), col(\"CIDADE\"), col(\"UF\"), col(\"CEP\"), col(\"TELEFONE\"),\n",
    "                     col(\"CNAE_DESCRICAO\"), col(\"EMAIL\"))\n",
    "# Salva o dataframe inicial\n",
    "dados_pandas_primario = dados_primario.toPandas()\n",
    "dados_pandas_primario.to_csv(arquivo_csv, sep=';', mode='a', index=False, encoding='utf-8')\n",
    "dados_pandas_primario.to_parquet(arquivo_parquet, sep=';', index=False, encoding='utf-8')\n",
    "\n",
    "# Chama a função de filtrar por cnae\n",
    "dados_secundario = filtra_cnpj_cnae_segundario()\n",
    "\n",
    "# Renomeia a coluna CNAE_DESCRICAO\n",
    "dados_secundario = dados_secundario.withColumnRenamed(\"DESCRICAO_CNAE\", \"CNAE_DESCRICAO\")\n",
    "\n",
    "# Seleciona as colunas\n",
    "dados_secundario = dados_secundario.select(col(\"CNPJ\"), col(\"RAZAO_SOCIAL\"), col(\"NOME_FANTASIA\"),\n",
    "                     col(\"SITUACAO_CADASTRAL\"), col(\"DATA_SITUACAO_CADASTRAL\"),\n",
    "                     col(\"DATA_INICIO_ATIVIDADE\"), col(\"CNAE_PRINCIPAL\"), col(\"ENDERECO\"),\n",
    "                     col(\"BAIRRO\"), col(\"CIDADE\"), col(\"UF\"), col(\"CEP\"), col(\"TELEFONE\"),\n",
    "                     col(\"CNAE_DESCRICAO\"), col(\"EMAIL\"))\n",
    "\n",
    "dados_pandas_segundario = dados_secundario.toPandas()\n",
    "dados_pandas_segundario.to_csv(arquivo_csv, sep=';', mode='a', index=False, encoding='utf-8')\n",
    "dados_pandas_segundario.to_parquet(arquivo_parquet, sep=';', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
